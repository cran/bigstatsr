% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sparseSVM.R
\name{big_spSVM}
\alias{big_spSVM}
\title{Sparse SVM}
\usage{
big_spSVM(X, y01.train, ind.train = rows_along(X), covar.train = NULL, ...)
}
\arguments{
\item{X}{A \link[=FBM-class]{FBM}.}

\item{y01.train}{Vector of responses, corresponding to \code{ind.train}.
\strong{Must be only 0s and 1s.}}

\item{ind.train}{An optional vector of the row indices that are used,
for the training part. If not specified, all rows are used.
\strong{Don't use negative indices.}}

\item{covar.train}{Matrix of covariables to be added in each model to correct
for confounders (e.g. the scores of PCA), corresponding to \code{ind.train}.
Default is \code{NULL} and corresponds to only adding an intercept to each model.}

\item{...}{Arguments passed on to \code{COPY_sparseSVM}
\describe{
  \item{alpha}{The elastic-net mixing parameter that controls the relative
contribution from the lasso and the ridge penalty. It must be a number
between \code{0} and \code{1}. \code{alpha=1} is the lasso penalty and \code{alpha=0}
the ridge penalty.}
  \item{gamma}{The tuning parameter for huberization smoothing of hinge loss.
Default is \code{0.1}.}
  \item{nlambda}{The number of lambda values. Default is \code{100}.}
  \item{lambda.min}{The smallest value for lambda, as a fraction of
\code{lambda.max}, the data derived entry value. Default is \code{0.01} if the number
of observations is larger than the number of variables and \code{0.05} otherwise.}
  \item{lambda}{A user-specified sequence of lambda values. Typical usage is
to leave blank and have the program automatically compute a \code{lambda}
sequence based on \code{nlambda} and \code{lambda.min}. Specifying
\code{lambda} overrides this. This argument should be used with care and
supplied with a decreasing sequence instead of a single value.}
  \item{screen}{Screening rule to be applied at each \code{lambda} that
discards variables for speed. Either "ASR" (default), "SR" or "none". "SR"
stands for the strong rule, and "ASR" for the adaptive strong rule. Using
"ASR" typically requires fewer iterations to converge than "SR", but the
computing time are generally close. Note that the option "none" is used
mainly for debugging, which may lead to much longer computing time.}
  \item{max.iter}{Maximum number of iterations. Default is 1000.}
  \item{eps}{Convergence threshold. The algorithms continue until the maximum
change in the objective after any coefficient update is less than \code{eps}
times the null deviance.  Default is \code{1E-7}.}
  \item{dfmax}{Upper bound for the number of nonzero coefficients. The
algorithm exits and returns a partial path if \code{dfmax} is reached.
Useful for very large dimensions.}
  \item{penalty.factor}{A numeric vector of length equal to the number of
variables. Each component multiplies \code{lambda} to allow differential
penalization. Can be 0 for some variables, in which case the variable is
always in the model without penalization. Default is \code{1} for all variables.}
  \item{message}{If set to \code{TRUE}, \code{sparseSVM} will inform the user of its
progress. Default is \code{FALSE}.}
}}
}
\value{
A named list containing:
\item{call}{The call that produced this object.}
\item{intercept}{A vector of intercepts, corresponding to each lambda.}
\item{beta}{The fitted matrix of coefficients. The number of rows is
equal to the number of coefficients, and the number of columns is equal
to \code{nlambda}.}
\item{iter}{A vector of length \code{nlambda} containing the number of
iterations until convergence at each value of \code{lambda}.}
\item{saturated}{A logical flag for whether the number of nonzero
coefficients has reached \code{dfmax}.}
\item{lambda}{The sequence of regularization parameter values in the path.}
\item{alpha}{Input parameter.}
\item{gamma}{Input parameter.}
\item{penalty.factor}{Input parameter.}
\item{levels}{Levels of the output class labels.}
}
\description{
Fit solution paths for sparse linear SVM regularized by lasso or elastic-net
over a grid of values for the regularization parameter lambda.
}
\details{
\strong{This is a modified version of one function of
\href{https://github.com/CY-dev/sparseSVM}{package sparseSVM}}.
This algorithm uses semi-newton coordinate descent. There may exist some
faster algorithm.

\strong{For now, this function is not exported anymore.} This is because it
doesn't match the results from package sparseSVM anymore, due to
\href{https://github.com/CY-dev/sparseSVM/pull/6}{a change} the author made.
I've contacted him multiple times, without success.
If you really want to use this function, use \code{bigstatsr:::big_spSVM}.
}
\examples{
set.seed(1)

big_spSVM <- bigstatsr:::big_spSVM

# simulating some data
N <- 73
M <- 430
X <- FBM(N, M, init = rnorm(N * M, sd = 5))
y <- sample(0:1, size = N, replace = TRUE)
y.factor <- factor(y, levels = c(1, 0))
covar <- matrix(rnorm(N * 3), N)

# error, only handle standard R matrices
\dontrun{test <- sparseSVM::sparseSVM(X, y)}

# OK here
test2 <- big_spSVM(X, y)
str(test2)

# how to use covariables?
X2 <- cbind(X[], covar)
test <- sparseSVM::sparseSVM(X2, y.factor, alpha = 0.5)
test2 <- big_spSVM(X, y, covar.train = covar, alpha = 0.5)
# verification
all.equal(test2$lambda, test$lambda)
all.equal(as.matrix(test2$beta), test$weights[-1, ], check.attributes = FALSE)
all.equal(test2$intercept, test$weights[1, ])
}
\seealso{
\link[LiblineaR:LiblineaR]{LiblineaR} \link[sparseSVM:sparseSVM]{sparseSVM}
}
